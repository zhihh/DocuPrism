#!/usr/bin/env python3
"""
å¤šè¿›ç¨‹æ—¥å¿—èšåˆå·¥å…·
å°†å„ä¸ªworkerè¿›ç¨‹çš„æ—¥å¿—åˆå¹¶ä¸ºç»Ÿä¸€è§†å›¾
"""

import os
import glob
import re
from datetime import datetime
from typing import List, Dict, Tuple
import argparse


class LogAggregator:
    """æ—¥å¿—èšåˆå™¨"""
    
    def __init__(self, logs_dir: str = "logs"):
        self.logs_dir = logs_dir
        self.timestamp_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})')
    
    def parse_log_line(self, line: str) -> Tuple[datetime, str]:
        """è§£ææ—¥å¿—è¡Œï¼Œæå–æ—¶é—´æˆ³"""
        match = self.timestamp_pattern.match(line.strip())
        if match:
            timestamp_str = match.group(1)
            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
            return timestamp, line.strip()
        return datetime.min, line.strip()
    
    def collect_worker_logs(self, log_type: str = "main") -> List[Tuple[datetime, str]]:
        """æ”¶é›†æ‰€æœ‰workerçš„æ—¥å¿—"""
        pattern = os.path.join(self.logs_dir, f"{log_type}_worker_*.log")
        log_files = glob.glob(pattern)
        
        if not log_files:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ—¥å¿—æ–‡ä»¶: {pattern}")
            return []
        
        all_logs = []
        
        for log_file in log_files:
            worker_id = re.search(r'worker_(\d+)', log_file)
            worker_name = f"Worker-{worker_id.group(1)}" if worker_id else "Unknown"
            
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            timestamp, content = self.parse_log_line(line)
                            # åœ¨æ¯è¡Œå‰æ·»åŠ workeræ ‡è¯†
                            marked_content = content.replace(f"PID-{worker_id.group(1)}", worker_name) if worker_id else content
                            all_logs.append((timestamp, marked_content))
            except Exception as e:
                print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        all_logs.sort(key=lambda x: x[0])
        return all_logs
    
    def aggregate_logs(self, log_type: str = "main", output_file: str = None):
        """èšåˆæ—¥å¿—å¹¶è¾“å‡º"""
        print(f"ğŸ”„ å¼€å§‹èšåˆ {log_type} æ—¥å¿—...")
        
        logs = self.collect_worker_logs(log_type)
        
        if not logs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ—¥å¿—æ•°æ®")
            return
        
        if output_file:
            output_path = os.path.join(self.logs_dir, output_file)
            with open(output_path, 'w', encoding='utf-8') as f:
                for timestamp, content in logs:
                    f.write(content + '\n')
            print(f"âœ… èšåˆæ—¥å¿—å·²ä¿å­˜åˆ°: {output_path}")
        else:
            # è¾“å‡ºåˆ°æ§åˆ¶å°
            for timestamp, content in logs:
                print(content)
        
        print(f"ğŸ“Š æ€»è®¡å¤„ç† {len(logs)} æ¡æ—¥å¿—è®°å½•")
    
    def real_time_aggregate(self, log_type: str = "main"):
        """å®æ—¶èšåˆæ—¥å¿—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        import time
        
        print(f"ğŸ”„ å¼€å§‹å®æ—¶ç›‘æ§ {log_type} æ—¥å¿—...")
        print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        last_positions = {}
        
        try:
            while True:
                pattern = os.path.join(self.logs_dir, f"{log_type}_worker_*.log")
                log_files = glob.glob(pattern)
                
                new_logs = []
                
                for log_file in log_files:
                    try:
                        # è®°å½•ä¸Šæ¬¡è¯»å–ä½ç½®
                        if log_file not in last_positions:
                            last_positions[log_file] = 0
                        
                        with open(log_file, 'r', encoding='utf-8') as f:
                            f.seek(last_positions[log_file])
                            new_lines = f.readlines()
                            last_positions[log_file] = f.tell()
                            
                            worker_id = re.search(r'worker_(\d+)', log_file)
                            worker_name = f"Worker-{worker_id.group(1)}" if worker_id else "Unknown"
                            
                            for line in new_lines:
                                if line.strip():
                                    timestamp, content = self.parse_log_line(line)
                                    marked_content = content.replace(f"PID-{worker_id.group(1)}", worker_name) if worker_id else content
                                    new_logs.append((timestamp, marked_content))
                    
                    except Exception as e:
                        print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
                
                # æ’åºå¹¶è¾“å‡ºæ–°æ—¥å¿—
                new_logs.sort(key=lambda x: x[0])
                for timestamp, content in new_logs:
                    print(content)
                
                time.sleep(1)  # 1ç§’æ£€æŸ¥ä¸€æ¬¡
                
        except KeyboardInterrupt:
            print("\nâœ… åœæ­¢å®æ—¶ç›‘æ§")
    
    def list_available_logs(self):
        """åˆ—å‡ºå¯ç”¨çš„æ—¥å¿—æ–‡ä»¶"""
        print("ğŸ“‹ å¯ç”¨çš„æ—¥å¿—æ–‡ä»¶:")
        
        patterns = ["main_worker_*.log", "api_access_worker_*.log", "document_processing_worker_*.log", "error_worker_*.log"]
        
        for pattern in patterns:
            files = glob.glob(os.path.join(self.logs_dir, pattern))
            if files:
                log_type = pattern.replace("_worker_*.log", "")
                print(f"  ğŸ“ {log_type}: {len(files)} ä¸ªworkeræ–‡ä»¶")
                for f in sorted(files):
                    size = os.path.getsize(f) / 1024  # KB
                    print(f"    - {os.path.basename(f)} ({size:.1f} KB)")
            else:
                log_type = pattern.replace("_worker_*.log", "")
                print(f"  ğŸ“ {log_type}: æ— workeræ–‡ä»¶")


def main():
    parser = argparse.ArgumentParser(description='å¤šè¿›ç¨‹æ—¥å¿—èšåˆå·¥å…·')
    parser.add_argument('--logs-dir', default='logs', help='æ—¥å¿—ç›®å½•è·¯å¾„')
    parser.add_argument('--type', default='main', choices=['main', 'api_access', 'document_processing', 'error'], 
                       help='æ—¥å¿—ç±»å‹')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--real-time', action='store_true', help='å®æ—¶ç›‘æ§æ¨¡å¼')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºå¯ç”¨æ—¥å¿—æ–‡ä»¶')
    
    args = parser.parse_args()
    
    aggregator = LogAggregator(args.logs_dir)
    
    if args.list:
        aggregator.list_available_logs()
    elif args.real_time:
        aggregator.real_time_aggregate(args.type)
    else:
        aggregator.aggregate_logs(args.type, args.output)


if __name__ == "__main__":
    main()