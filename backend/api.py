"""
æ–‡æ¡£é¢„å¤„ç†API
ä¸ºç°æœ‰çš„æŸ¥é‡APIæ·»åŠ æ–‡æ¡£å¤„ç†åŠŸèƒ½
"""

import os
import uuid
import time
import logging
import tempfile
from typing import List, Optional, Dict, Any
from pathlib import Path

from fastapi import FastAPI, UploadFile, File, HTTPException, Form, Request
from fastapi.responses import JSONResponse

from .processors.document_processor import DocumentProcessor
from .models.document_models import DocumentParseResult, TextBlock
from .utils.logger import get_document_logger, log_api_access, log_api_result, log_processing_performance

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
try:
    from src.utils.unified_logger import UnifiedLogger
    unified_logger = UnifiedLogger.get_logger('document_api')
except ImportError:
    # å¦‚æœç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨åŸæœ‰çš„logger
    unified_logger = get_document_logger('api')

logger = unified_logger


class DocumentProcessingAPI:
    """æ–‡æ¡£é¢„å¤„ç†APIç±»"""
    
    def __init__(self, app: FastAPI, enable_ocr: bool = True):
        """
        åˆå§‹åŒ–API
        
        Args:
            app: FastAPIåº”ç”¨å®ä¾‹
            enable_ocr: æ˜¯å¦å¯ç”¨OCR
        """
        self.app = app
        self.processor = DocumentProcessor(enable_ocr=enable_ocr)
        
        # æ³¨å†ŒAPIè·¯ç”±
        self._register_routes()
        
        logger.info(f"æ–‡æ¡£é¢„å¤„ç†APIå·²åˆå§‹åŒ–ï¼ŒOCRåŠŸèƒ½: {'å·²å¯ç”¨' if self.processor.is_ocr_enabled() else 'å·²ç¦ç”¨'}")
    
    def _register_routes(self):
        """æ³¨å†ŒAPIè·¯ç”±"""
        
        @self.app.post("/api/v2/upload-document")
        async def upload_and_process_document(
            request: Request,
            file: UploadFile = File(...),
            document_id: Optional[str] = Form(None)
        ):
            """
            ä¸Šä¼ å¹¶å¤„ç†å•ä¸ªæ–‡æ¡£
            
            Returns:
                DocumentParseResult: æ–‡æ¡£è§£æç»“æœ
            """
            start_time = time.time()
            client_ip = request.client.host if request.client else None
            user_agent = request.headers.get("user-agent")
            
            try:
                # è®°å½•APIè¯·æ±‚
                files_info = [{"name": file.filename, "size": file.size, "type": file.content_type}]
                log_api_access("/api/v2/upload-document", "POST", files_info, client_ip, user_agent)
                
                logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†å•ä¸ªæ–‡æ¡£: {file.filename} (å¤§å°: {file.size} å­—èŠ‚)")
                
                # éªŒè¯æ–‡ä»¶ç±»å‹
                if not file.filename:
                    raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
                
                file_extension = Path(file.filename).suffix.lower()
                supported_formats = self.processor.get_supported_formats()
                
                if file_extension not in supported_formats:
                    raise HTTPException(
                        status_code=400, 
                        detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æŒçš„æ ¼å¼: {list(supported_formats.keys())}"
                    )
                
                # ç”Ÿæˆæ–‡æ¡£ID
                if not document_id:
                    document_id = str(uuid.uuid4())
                
                logger.info(f"ğŸ” å¼€å§‹å¤„ç†æ–‡æ¡£: {file.filename}, ID: {document_id}")
                
                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as tmp_file:
                    content = await file.read()
                    tmp_file.write(content)
                    tmp_path = tmp_file.name
                
                logger.debug(f"ğŸ“ ä¸´æ—¶æ–‡ä»¶ä¿å­˜è‡³: {tmp_path}")
                
                try:
                    # å¤„ç†æ–‡æ¡£
                    if file_extension in ['.png', '.jpg', '.jpeg']:
                        # å›¾ç‰‡æ–‡ä»¶ä½¿ç”¨OCR
                        if not self.processor.is_ocr_enabled():
                            raise HTTPException(status_code=400, detail="OCRåŠŸèƒ½æœªå¯ç”¨ï¼Œæ— æ³•å¤„ç†å›¾ç‰‡æ–‡ä»¶")
                        logger.info(f"ğŸ–¼ï¸ ä½¿ç”¨OCRå¤„ç†å›¾ç‰‡: {file.filename}")
                        result = self.processor.process_image_with_ocr(tmp_path, document_id)
                    else:
                        # æ–‡æ¡£æ–‡ä»¶
                        logger.info(f"ğŸ“‹ å¤„ç†æ–‡æ¡£æ–‡ä»¶: {file.filename}")
                        result = self.processor.process_file(tmp_path, document_id)
                    
                    processing_time = time.time() - start_time
                    
                    # è®°å½•å¤„ç†æ€§èƒ½
                    log_processing_performance(
                        file_type=file_extension,
                        file_size=file.size or 0,
                        processing_time=processing_time,
                        text_blocks=len(result.all_text_blocks),
                        ocr_used=file_extension in ['.png', '.jpg', '.jpeg']
                    )
                    
                    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                    response_data = self._serialize_parse_result(result)
                    
                    logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {file.filename}, è€—æ—¶: {processing_time:.2f}ç§’, æ–‡æœ¬å—: {len(result.all_text_blocks)}ä¸ª")
                    
                    # è®°å½•APIç»“æœ
                    result_summary = {
                        "document_id": document_id,
                        "file_name": file.filename,
                        "text_blocks": len(result.all_text_blocks),
                        "total_text_length": result.total_text_length,
                        "processing_time": processing_time
                    }
                    log_api_result("/api/v2/upload-document", "POST", 200, processing_time, result_summary)
                    
                    return JSONResponse(content={
                        "success": True,
                        "message": "æ–‡æ¡£å¤„ç†æˆåŠŸ",
                        "data": response_data
                    })
                    
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    Path(tmp_path).unlink(missing_ok=True)
                    logger.debug(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {tmp_path}")
                    
            except HTTPException:
                processing_time = time.time() - start_time
                log_api_result("/api/v2/upload-document", "POST", 400, processing_time, {"error": "HTTPå¼‚å¸¸"})
                raise
            except Exception as e:
                processing_time = time.time() - start_time
                logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {file.filename if file and file.filename else 'unknown'}, é”™è¯¯: {e}")
                log_api_result("/api/v2/upload-document", "POST", 500, processing_time, {"error": str(e)})
                raise HTTPException(status_code=500, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
        
        @self.app.post("/api/v2/upload-and-analyze")
        async def upload_and_analyze_documents(
            request: Request,
            files: List[UploadFile] = File(...),
            threshold: float = Form(0.7),
            method: str = Form("semantic")
        ):
            """
            ä¸Šä¼ å¤šä¸ªæ–‡æ¡£å¹¶ç›´æ¥è¿›è¡ŒæŸ¥é‡åˆ†æ
            
            Returns:
                æŸ¥é‡åˆ†æç»“æœ
            """
            start_time = time.time()
            client_ip = request.client.host if request.client else None
            user_agent = request.headers.get("user-agent")
            
            try:
                # è®°å½•APIè¯·æ±‚
                files_info = [{"name": f.filename, "size": f.size, "type": f.content_type} for f in files]
                log_api_access("/api/v2/upload-and-analyze", "POST", files_info, client_ip, user_agent)
                
                if len(files) < 2:
                    raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦ä¸Šä¼ 2ä¸ªæ–‡æ¡£è¿›è¡Œæ¯”å¯¹")
                
                logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(files)} ä¸ªæ–‡æ¡£è¿›è¡ŒæŸ¥é‡åˆ†æ")
                logger.info(f"ğŸ“Š åˆ†æå‚æ•°: threshold={threshold}, method={method}")
                
                # å¤„ç†æ‰€æœ‰æ–‡æ¡£
                processed_documents = []
                temp_files = []
                
                try:
                    for i, file in enumerate(files):
                        if not file.filename:
                            logger.warning(f"âš ï¸ è·³è¿‡æ— åæ–‡ä»¶ (ç´¢å¼• {i})")
                            continue
                        
                        file_extension = Path(file.filename).suffix.lower()
                        supported_formats = self.processor.get_supported_formats()
                        
                        if file_extension not in supported_formats:
                            logger.warning(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file.filename} ({file_extension})")
                            continue
                        
                        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i+1}/{len(files)}: {file.filename} ({file_extension})")
                        
                        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                        with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as tmp_file:
                            content = await file.read()
                            tmp_file.write(content)
                            tmp_path = tmp_file.name
                            temp_files.append(tmp_path)
                        
                        # å¤„ç†æ–‡æ¡£
                        document_id = f"doc_{i+1}"
                        
                        if file_extension in ['.png', '.jpg', '.jpeg']:
                            if self.processor.is_ocr_enabled():
                                logger.info(f"ğŸ–¼ï¸ ä½¿ç”¨OCRå¤„ç†å›¾ç‰‡: {file.filename}")
                                result = self.processor.process_image_with_ocr(tmp_path, document_id)
                            else:
                                logger.warning(f"âš ï¸ OCRæœªå¯ç”¨ï¼Œè·³è¿‡å›¾ç‰‡æ–‡ä»¶: {file.filename}")
                                continue
                        else:
                            logger.info(f"ğŸ“‹ å¤„ç†æ–‡æ¡£: {file.filename}")
                            result = self.processor.process_file(tmp_path, document_id)
                        
                        # è®°å½•å¤„ç†æ€§èƒ½
                        file_processing_time = time.time() - start_time
                        log_processing_performance(
                            file_type=file_extension,
                            file_size=file.size or 0,
                            processing_time=file_processing_time,
                            text_blocks=len(result.all_text_blocks),
                            ocr_used=file_extension in ['.png', '.jpg', '.jpeg']
                        )
                        
                        logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file.filename}, æ–‡æœ¬å—: {len(result.all_text_blocks)}ä¸ª")
                        
                        # è½¬æ¢ä¸ºæŸ¥é‡APIéœ€è¦çš„æ ¼å¼
                        for block in result.all_text_blocks:
                            processed_documents.append({
                                "documentId": int(document_id.split('_')[1]),
                                "page": block.page_number,
                                "content": block.text
                            })
                    
                    if len(processed_documents) < 2:
                        raise HTTPException(status_code=400, detail="æˆåŠŸå¤„ç†çš„æ–‡æ¡£å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡Œæ¯”å¯¹")
                    
                    logger.info(f"ğŸ“ˆ å¼€å§‹æŸ¥é‡åˆ†æï¼Œå…± {len(processed_documents)} ä¸ªæ–‡æ¡£å—")
                    
                    # ä¸´æ—¶è®¾ç½®åŸæœ‰APIçš„æ—¥å¿—çº§åˆ«
                    original_service_logger = logging.getLogger('src.api.service')
                    original_level = original_service_logger.level
                    original_service_logger.setLevel(logging.INFO)
                    
                    # ä¸ºåŸæœ‰APIæ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
                    console_handler = logging.StreamHandler()
                    console_handler.setLevel(logging.INFO)
                    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s', datefmt='%H:%M:%S')
                    console_handler.setFormatter(formatter)
                    original_service_logger.addHandler(console_handler)
                    
                    try:
                        # è°ƒç”¨ç°æœ‰çš„æŸ¥é‡æœåŠ¡
                        from src.api.service import DocumentDeduplicationService
                        dedup_service = DocumentDeduplicationService()
                        
                        logger.info(f"ğŸ” è°ƒç”¨åŸæœ‰æŸ¥é‡æœåŠ¡è¿›è¡Œåˆ†æ...")
                        duplicate_results = await dedup_service.analyze_documents(processed_documents)
                        logger.info(f"ğŸ“Š åŸæœ‰æŸ¥é‡æœåŠ¡è¿”å› {len(duplicate_results)} ä¸ªç»“æœ")
                        
                    finally:
                        # æ¢å¤åŸå§‹æ—¥å¿—è®¾ç½®
                        original_service_logger.removeHandler(console_handler)
                        original_service_logger.setLevel(original_level)
                    
                    processing_time = time.time() - start_time
                    
                    # æ ¼å¼åŒ–ç»“æœ
                    formatted_results = []
                    for result in duplicate_results:
                        formatted_results.append({
                            "doc1_id": result.documentId1,
                            "doc2_id": result.documentId2,
                            "page1": result.page1,
                            "page2": result.page2,
                            "content1": result.content1,
                            "content2": result.content2,
                            "similarity": result.score,
                            "reason": result.reason,
                            "category": result.category
                        })
                    
                    logger.info(f"ğŸ¯ æŸ¥é‡åˆ†æå®Œæˆ! å‘ç° {len(formatted_results)} å¯¹é‡å¤å†…å®¹ï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
                    
                    # è®°å½•APIç»“æœ
                    result_summary = {
                        "files_processed": len([f for f in files if f.filename]),
                        "document_blocks": len(processed_documents),
                        "duplicates_found": len(formatted_results),
                        "processing_time": processing_time,
                        "threshold": threshold,
                        "method": method
                    }
                    log_api_result("/api/v2/upload-and-analyze", "POST", 200, processing_time, result_summary)
                    
                    return JSONResponse(content={
                        "success": True,
                        "message": f"åˆ†æå®Œæˆï¼Œå‘ç° {len(formatted_results)} å¯¹é‡å¤å†…å®¹",
                        "data": formatted_results,
                        "total_count": len(formatted_results),
                        "processed_documents": len(processed_documents),
                        "processing_time": processing_time,
                        "config": {
                            "threshold": threshold,
                            "method": method
                        }
                    })
                    
                finally:
                    # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
                    for tmp_path in temp_files:
                        Path(tmp_path).unlink(missing_ok=True)
                    logger.debug(f"ğŸ—‘ï¸ æ¸…ç†äº† {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶")
                    
            except HTTPException:
                processing_time = time.time() - start_time
                log_api_result("/api/v2/upload-and-analyze", "POST", 400, processing_time, {"error": "HTTPå¼‚å¸¸"})
                raise
            except Exception as e:
                processing_time = time.time() - start_time
                logger.error(f"âŒ æ‰¹é‡æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: {processing_time:.2f}ç§’, é”™è¯¯: {e}")
                log_api_result("/api/v2/upload-and-analyze", "POST", 500, processing_time, {"error": str(e)})
                raise HTTPException(status_code=500, detail=f"æ‰¹é‡æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
        
        @self.app.get("/api/v2/supported-formats")
        async def get_supported_formats():
            """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"""
            formats = self.processor.get_supported_formats()
            
            return JSONResponse(content={
                "success": True,
                "data": {
                    "formats": formats,
                    "ocr_enabled": self.processor.is_ocr_enabled(),
                    "ocr_formats": [".png", ".jpg", ".jpeg"] if self.processor.is_ocr_enabled() else []
                }
            })
        
        @self.app.get("/api/v2/processor-status")
        async def get_processor_status():
            """è·å–æ–‡æ¡£å¤„ç†å™¨çŠ¶æ€"""
            return JSONResponse(content={
                "success": True,
                "data": {
                    "ocr_enabled": self.processor.is_ocr_enabled(),
                    "supported_formats": self.processor.get_supported_formats(),
                    "version": "1.0.0"
                }
            })
    
    def _serialize_parse_result(self, result: DocumentParseResult) -> Dict[str, Any]:
        """å°†DocumentParseResultåºåˆ—åŒ–ä¸ºå¯JSONåŒ–çš„å­—å…¸"""
        return {
            "document_id": result.document_id,
            "document_type": result.document_type.value,
            "total_pages": result.total_pages,
            "total_text_length": result.total_text_length,
            "processing_time": result.processing_time,
            "metadata": result.metadata,
            "pages": [
                {
                    "page_number": page.page_number,
                    "width": page.width,
                    "height": page.height,
                    "has_images": page.has_images,
                    "image_count": page.image_count,
                    "text_blocks_count": len(page.text_blocks)
                }
                for page in result.pages
            ],
            "text_blocks": [
                {
                    "text": block.text,
                    "page_number": block.page_number,
                    "block_id": block.block_id,
                    "original_position": block.original_position,
                    "char_start": block.char_start,
                    "char_end": block.char_end,
                    "confidence": block.confidence,
                    "block_type": block.block_type,
                    "is_ocr": block.is_ocr
                }
                for block in result.all_text_blocks
            ]
        }