"""
DocuPrism AI - æ–‡æ¡£æ™ºèƒ½æ¯”å¯¹æ ¸å¿ƒæœåŠ¡
AI-Powered Semantic Document Comparison Platform

æ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„æœåŠ¡æ¥å£ï¼Œæ”¯æŒå¹¶å‘å¤„ç†
"""

import time
import asyncio
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.schema.runnable import RunnableLambda, RunnableParallel

from ..models.api_models import DuplicateOutput
from ..models.data_models import DocumentData
from ..core.document_processor import DocumentProcessor
from ..core.clustering_manager import ClusteringManager
from ..detectors.llm_duplicate_detector import LLMDuplicateDetector
from ..validators.validation_manager import ValidationManager
from ..config.config import Config
from ..utils.unified_logger import UnifiedLogger

logger = UnifiedLogger.get_logger(__name__)


class DocumentDeduplicationService:
    """æ–‡æ¡£æ™ºèƒ½æ¯”å¯¹æœåŠ¡ - é«˜å¹¶å‘ç‰ˆæœ¬"""
    
    def __init__(self):
        self.config = Config()
        self.processor = DocumentProcessor()
        
        # ä½¿ç”¨é…ç½®åˆå§‹åŒ–èšç±»ç®¡ç†å™¨
        self.clustering_manager = ClusteringManager(
            top_k=self.config.top_k_candidates,
            similarity_threshold=self.config.similarity_threshold,
            use_reranker=self.config.use_reranker,
            max_candidates_for_rerank=self.config.max_rerank_candidates
        )
        
        self.detector = LLMDuplicateDetector()
        self.validator = ValidationManager()
        # ç§»é™¤å…¨å±€é”ï¼Œæ”¯æŒå¹¶å‘å¤„ç†
        self.max_workers = 4  # å¯æ ¹æ®æœåŠ¡å™¨é…ç½®è°ƒæ•´
        
        logger.info(f"æ–‡æ¡£æ™ºèƒ½æ¯”å¯¹æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨å¢å¼ºç‰ˆèšç±»ç­–ç•¥")
    
    async def analyze_documents(self, json_input: List[Dict]) -> List[DuplicateOutput]:
        """åˆ†ææ–‡æ¡£é‡å¤å†…å®¹ - é«˜å¹¶å‘å¼‚æ­¥å¤„ç†ç‰ˆæœ¬"""
        
        execution_id = int(time.time() * 1000)
        start_time = time.time()
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ (ID: {execution_id})")
        
        try:
            # 1. å¤„ç†è¾“å…¥æ•°æ®
            logger.info(f"[{execution_id}] ğŸ“„ æ­£åœ¨å¤„ç†JSONè¾“å…¥ï¼Œæ–‡æ¡£æ•°é‡: {len(json_input)}")
            process_start = time.time()
            document_data_list, document_inputs = await self._run_in_executor(
                self.processor.process_json_documents, json_input
            )
            process_time = time.time() - process_start
            logger.info(f"[{execution_id}] âœ… JSONå¤„ç†å®Œæˆï¼Œè€—æ—¶: {process_time:.2f}ç§’ï¼Œç”Ÿæˆ{len(document_data_list)}ä¸ªæ–‡æ¡£å—")
            
            # 2. å¹¶è¡Œæ‰§è¡Œä¸¤ç§ç­–ç•¥
            logger.info(f"[{execution_id}] ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œåˆ†å‰²èšç±»æŸ¥é‡å’Œç›´æ¥æŸ¥é‡...")
            strategy_start = time.time()
            
            # ä½¿ç”¨asyncioåˆ›å»ºå¹¶å‘ä»»åŠ¡
            logger.info(f"[{execution_id}] ğŸ”§ åˆ›å»ºèšç±»ä»»åŠ¡")
            cluster_task = asyncio.create_task(
                self._clustering_strategy(execution_id, document_inputs)
            )
            logger.info(f"[{execution_id}] ğŸ”§ åˆ›å»ºç›´æ¥ç­–ç•¥ä»»åŠ¡")
            direct_task = asyncio.create_task(
                self._direct_strategy(execution_id, document_data_list)
            )
            
            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
            logger.info(f"[{execution_id}] â³ ç­‰å¾…å¹¶è¡Œä»»åŠ¡å®Œæˆ...")
            cluster_results, direct_results = await asyncio.gather(
                cluster_task, 
                direct_task, 
                return_exceptions=True
            )
            
            strategy_time = time.time() - strategy_start
            logger.info(f"[{execution_id}] âš¡ å¹¶è¡Œç­–ç•¥æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {strategy_time:.2f}ç§’")
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            if isinstance(cluster_results, Exception):
                logger.error(f"[{execution_id}] âŒ èšç±»ç­–ç•¥å¤±è´¥: {cluster_results}")
                cluster_results = []
            
            if isinstance(direct_results, Exception):
                logger.error(f"[{execution_id}] âŒ ç›´æ¥ç­–ç•¥å¤±è´¥: {direct_results}")
                direct_results = []
            
            # ç¡®ä¿ç»“æœæ˜¯åˆ—è¡¨ç±»å‹
            cluster_results = cluster_results if isinstance(cluster_results, list) else []
            direct_results = direct_results if isinstance(direct_results, list) else []
            
            # 3. åˆå¹¶å¹¶å»é‡ç»“æœ
            logger.info(f"[{execution_id}] ğŸ“Š åˆå¹¶ç»“æœï¼šèšç±» {len(cluster_results)} + ç›´æ¥ {len(direct_results)}")
            merge_start = time.time()
            combined_results = cluster_results + direct_results
            unique_results = self._deduplicate_results(combined_results)
            merge_time = time.time() - merge_start
            logger.info(f"[{execution_id}] ğŸ”„ ç»“æœåˆå¹¶å»é‡å®Œæˆï¼Œè€—æ—¶: {merge_time:.3f}ç§’ï¼Œæœ€ç»ˆ {len(unique_results)} å¯¹é‡å¤å†…å®¹")
            
            # 4. éªŒè¯ç»“æœ
            if unique_results:
                logger.info(f"[{execution_id}] ğŸ” å¼€å§‹éªŒè¯æ£€æµ‹ç»“æœ...")
                validation_start = time.time()
                validated_results = await self._run_in_executor(
                    self.validator.validate_results, document_data_list, unique_results
                )
                validation_time = time.time() - validation_start
                logger.info(f"[{execution_id}] âœ… éªŒè¯å®Œæˆï¼Œè€—æ—¶: {validation_time:.2f}ç§’ï¼Œæœ€ç»ˆç»“æœ: {len(validated_results)} å¯¹é‡å¤å†…å®¹")
            else:
                validated_results = []
                logger.info(f"[{execution_id}] â„¹ï¸ æ— æ£€æµ‹ç»“æœéœ€è¦éªŒè¯")
            
            total_time = time.time() - start_time
            logger.info(f"[{execution_id}] ğŸ‰ å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            return validated_results
            
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"[{execution_id}] âŒ æ–‡æ¡£åˆ†æå¤±è´¥ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
            raise
    
    async def _clustering_strategy(self, execution_id: int, document_inputs) -> List[DuplicateOutput]:
        """åˆ†å‰²èšç±»æŸ¥é‡ç­–ç•¥ - å¼‚æ­¥ç‰ˆæœ¬"""
        strategy_start = time.time()
        try:
            # åˆ†å‰²æ–‡æ¡£
            logger.info(f"[{execution_id}] ğŸ” èšç±»ç­–ç•¥ï¼šå¼€å§‹åˆ†å‰²æ–‡æ¡£...")
            segment_start = time.time()
            segments = await self._run_in_executor(
                self.processor.segment_documents, document_inputs
            )
            segment_time = time.time() - segment_start
            logger.info(f"[{execution_id}] âœ… èšç±»ç­–ç•¥ï¼šå·²åˆ†å‰²å‡º {len(segments)} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œè€—æ—¶: {segment_time:.2f}ç§’")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info(f"[{execution_id}] ğŸ§  èšç±»ç­–ç•¥ï¼šå¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")
            embedding_start = time.time()
            segments = await self._run_in_executor(
                self.processor.generate_embeddings, segments
            )
            embedding_time = time.time() - embedding_start
            logger.info(f"[{execution_id}] âœ… èšç±»ç­–ç•¥ï¼šå·²ç”Ÿæˆ {len(segments)} ä¸ªåµŒå…¥å‘é‡ï¼Œè€—æ—¶: {embedding_time:.2f}ç§’")
            
            # èšç±»åˆ†æ
            logger.info(f"[{execution_id}] ğŸ¯ èšç±»ç­–ç•¥ï¼šå¼€å§‹èšç±»åˆ†æ...")
            cluster_start = time.time()
            clusters = await self._run_in_executor(
                self.clustering_manager.initial_clustering, segments
            )
            multi_doc_clusters = await self._run_in_executor(
                self.clustering_manager.filter_multi_document_clusters, clusters
            )
            cluster_time = time.time() - cluster_start
            logger.info(f"[{execution_id}] âœ… èšç±»ç­–ç•¥ï¼šå‘ç° {len(multi_doc_clusters)} ä¸ªå¯èƒ½åŒ…å«é‡å¤å†…å®¹çš„èšç±»ï¼Œè€—æ—¶: {cluster_time:.2f}ç§’")
            
            # æ£€æµ‹é‡å¤å†…å®¹
            logger.info(f"[{execution_id}] ğŸ¤– èšç±»ç­–ç•¥ï¼šå¼€å§‹LLMæ£€æµ‹...")
            llm_start = time.time()
            if multi_doc_clusters:
                cluster_results = await self._run_in_executor(
                    self.detector.detect_duplicates_parallel, multi_doc_clusters
                )
            else:
                cluster_results = []
            llm_time = time.time() - llm_start
            strategy_time = time.time() - strategy_start
            
            logger.info(f"[{execution_id}] âœ… èšç±»ç­–ç•¥ï¼šå‘ç° {len(cluster_results)} å¯¹é‡å¤å†…å®¹ï¼ŒLLMè€—æ—¶: {llm_time:.2f}ç§’ï¼Œæ€»è€—æ—¶: {strategy_time:.2f}ç§’")
            return cluster_results
            
        except Exception as e:
            strategy_time = time.time() - strategy_start
            logger.error(f"[{execution_id}] âŒ èšç±»ç­–ç•¥å¤±è´¥ï¼Œè€—æ—¶: {strategy_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
            return []
    
    async def _direct_strategy(self, execution_id: int, document_data_list) -> List[DuplicateOutput]:
        """ç›´æ¥æŸ¥é‡ç­–ç•¥ - å¼‚æ­¥ç‰ˆæœ¬"""
        strategy_start = time.time()
        try:
            logger.info(f"[{execution_id}] ğŸ¯ ç›´æ¥ç­–ç•¥ï¼šå¼€å§‹å®Œæ•´æ–‡æ¡£æ¯”è¾ƒï¼Œæ–‡æ¡£æ•°é‡: {len(document_data_list)}")
            direct_results = await self._run_in_executor(
                self.detector.direct_document_comparison, document_data_list
            )
            strategy_time = time.time() - strategy_start
            logger.info(f"[{execution_id}] âœ… ç›´æ¥ç­–ç•¥ï¼šå‘ç° {len(direct_results)} å¯¹é‡å¤å†…å®¹ï¼Œè€—æ—¶: {strategy_time:.2f}ç§’")
            return direct_results
            
        except Exception as e:
            strategy_time = time.time() - strategy_start
            logger.error(f"[{execution_id}] âŒ ç›´æ¥ç­–ç•¥å¤±è´¥ï¼Œè€—æ—¶: {strategy_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
            return []
    
    async def _run_in_executor(self, func, *args):
        """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°"""
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            return await loop.run_in_executor(executor, func, *args)
    
    def _deduplicate_results(self, results: List[DuplicateOutput]) -> List[DuplicateOutput]:
        """å»é™¤é‡å¤çš„æ£€æµ‹ç»“æœ"""
        if not results:
            return results
        
        logger.info(f"ğŸ”„ å¼€å§‹å»é‡å¤„ç†ï¼Œè¾“å…¥ {len(results)} å¯¹ç»“æœ...")
        unique_results = []
        seen_pairs = set()
        
        for result in results:
            # åˆ›å»ºæ ‡å‡†åŒ–çš„å†…å®¹å¯¹æ ‡è¯†
            content_pair = tuple(sorted([
                result.content1.strip().lower(),
                result.content2.strip().lower()
            ]))
            
            if content_pair not in seen_pairs:
                seen_pairs.add(content_pair)
                unique_results.append(result)
        
        logger.info(f"âœ… å»é‡å®Œæˆï¼Œå»é‡å‰: {len(results)} å¯¹ï¼Œå»é‡å: {len(unique_results)} å¯¹")
        return unique_results
        
        logger.info(f"å»é‡å‰: {len(results)} å¯¹ï¼Œå»é‡å: {len(unique_results)} å¯¹")
        return unique_results
