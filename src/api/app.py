"""
DocuPrism AI - FastAPIåº”ç”¨å®šä¹‰
AI-Powered Semantic Document Comparison Platform

åŒ…å«æ‰€æœ‰APIè·¯ç”±å’Œä¸­é—´ä»¶é…ç½®
"""

import logging
import json
from datetime import datetime
from typing import List, Any, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware

from ..models.api_models import DocumentInput, ApiResponse
from .service import DocumentDeduplicationService
from ..config.config import Config

logger = logging.getLogger(__name__)

# åˆ›å»ºé…ç½®å®ä¾‹
config = Config()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="DocuPrism AI - åŸºäºAIè¯­ä¹‰ç†è§£çš„æ–‡æ¡£æ™ºèƒ½æ¯”å¯¹ç³»ç»Ÿ",
    description="AI-Powered Semantic Document Comparison Platform",
    version="2.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æœåŠ¡
deduplication_service = DocumentDeduplicationService()


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "DocuPrism AI - åŸºäºAIè¯­ä¹‰ç†è§£çš„æ–‡æ¡£æ™ºèƒ½æ¯”å¯¹ç³»ç»Ÿ",
        "version": "2.0.0",
        "status": "running"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.post("/analyze")
async def analyze_documents_simple(request: Request):
    """
    ç®€åŒ–ç‰ˆåˆ†ææ–‡æ¡£é‡å¤å†…å®¹æ¥å£
    ä¸ºå‰ç«¯ç•Œé¢æä¾›ç®€å•çš„è°ƒç”¨æ–¹å¼
    """
    start_time = datetime.now()
    
    try:
        # è·å–è¯·æ±‚ä½“
        request_body = await request.body()
        request_data = json.loads(request_body.decode('utf-8'))
        
        # è·å–documentsæ•°æ®
        documents_data = request_data.get('documents', [])
        threshold = request_data.get('threshold', 0.7)
        method = request_data.get('method', 'semantic')
        
        logger.info(f"ğŸ“¨ ç®€åŒ–æ¥å£æ”¶åˆ°åˆ†æè¯·æ±‚ï¼Œæ–‡æ¡£æ•°é‡: {len(documents_data)}")
        logger.info(f"âš™ï¸ é˜ˆå€¼: {threshold}, æ–¹æ³•: {method}")
        
        if not documents_data:
            raise HTTPException(status_code=400, detail="è¾“å…¥æ–‡æ¡£ä¸èƒ½ä¸ºç©º")
        
        # è½¬æ¢ä¸ºDocumentInputæ ¼å¼
        documents = []
        for doc_data in documents_data:
            doc = DocumentInput(
                documentId=doc_data.get('document_id'),
                page=doc_data.get('page', 1),
                content=doc_data.get('content', '')
            )
            documents.append(doc)
        
        # è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼
        json_input = [
            {
                "documentId": doc.documentId,
                "page": doc.page,
                "content": doc.content
            }
            for doc in documents
        ]
        
        # æ‰§è¡Œåˆ†æ
        duplicate_results = await deduplication_service.analyze_documents(json_input)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_comparisons = len(documents) * (len(documents) - 1) // 2
        duplicates_found = len(duplicate_results)
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_duplicates = []
        for result in duplicate_results:
            formatted_duplicates.append({
                "doc1_id": result.documentId1,
                "doc2_id": result.documentId2,
                "similarity": result.score,
                "content1": result.content1,
                "content2": result.content2
            })
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"âœ… ç®€åŒ–æ¥å£åˆ†æå®Œæˆï¼Œå‘ç° {duplicates_found} å¯¹é‡å¤å†…å®¹ï¼Œè€—æ—¶ {processing_time:.2f}ç§’")
        
        # è¿”å›å‰ç«¯æœŸæœ›çš„æ ¼å¼
        return {
            "success": True,
            "message": f"åˆ†æå®Œæˆï¼Œå‘ç° {duplicates_found} å¯¹é‡å¤å†…å®¹",
            "data": formatted_duplicates,  # æ·»åŠ dataå­—æ®µ
            "total_comparisons": total_comparisons,
            "duplicates_found": duplicates_found,
            "duplicates": formatted_duplicates,
            "processing_time": processing_time,
            "config": {
                "threshold": threshold,
                "method": method
            }
        }
        
    except HTTPException as he:
        logger.error(f"âŒ HTTPå¼‚å¸¸: {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"âŒ ç®€åŒ–æ¥å£è°ƒç”¨å¤±è´¥: {e}")
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "success": False,
            "message": f"åˆ†æå¤±è´¥: {str(e)}",
            "total_comparisons": 0,
            "duplicates_found": 0,
            "duplicates": [],
            "processing_time": processing_time
        }


@app.post("/api/v2/analyze", response_model=ApiResponse)
async def analyze_documents(request: Request, documents: List[DocumentInput]):
    """
    åˆ†ææ–‡æ¡£é‡å¤å†…å®¹ - å¼‚æ­¥å¹¶å‘ç‰ˆæœ¬
    
    è¾“å…¥æ ¼å¼:
    [
        {
            "documentId": 1,
            "page": 1,
            "content": "æ–‡æ¡£å†…å®¹"
        }
    ]
    
    è¾“å‡ºæ ¼å¼åŒ…å«é‡å¤å†…å®¹å¯¹çš„è¯¦ç»†ä¿¡æ¯
    """
    start_time = datetime.now()
    
    # è·å–åŸå§‹è¯·æ±‚ä½“ç”¨äºè°ƒè¯•ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
    if config.debug_request_body:
        try:
            request_body = await request.body()
            request_text = request_body.decode('utf-8')
            logger.info(f"ğŸ” [DEBUG] æ”¶åˆ°è¯·æ±‚ï¼ŒåŸå§‹è¯·æ±‚ä½“: {request_text}")
            logger.info(f"ğŸ“Š [DEBUG] è¯·æ±‚å¤´: {dict(request.headers)}")
            logger.info(f"ğŸ¯ [DEBUG] è§£æåçš„æ–‡æ¡£æ•°é‡: {len(documents)}")
            
            # è®°å½•è§£æåçš„æ–‡æ¡£ç»“æ„
            for i, doc in enumerate(documents[:3]):  # åªè®°å½•å‰3ä¸ªæ–‡æ¡£é¿å…æ—¥å¿—è¿‡é•¿
                logger.info(f"ğŸ“„ [DEBUG] æ–‡æ¡£ {i+1}: documentId={doc.documentId}, page={doc.page}, contenté•¿åº¦={len(doc.content)}")
                logger.info(f"ğŸ“„ [DEBUG] æ–‡æ¡£ {i+1} å†…å®¹å‰100å­—ç¬¦: {doc.content[:100]}...")
                
        except Exception as e:
            logger.error(f"âŒ [DEBUG] è·å–è¯·æ±‚ä½“å¤±è´¥: {e}")
    else:
        logger.info(f"ğŸ“¨ æ”¶åˆ°åˆ†æè¯·æ±‚ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
        # é‡æ–°è¯»å–è¯·æ±‚ä½“ç”¨äºé‡æ–°æ„é€ requestå¯¹è±¡ï¼ˆå› ä¸ºbodyåªèƒ½è¯»å–ä¸€æ¬¡ï¼‰
        # è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å·²è§£æçš„documents
    
    try:
        # éªŒè¯è¾“å…¥
        if not documents:
            logger.warning("âš ï¸ è¾“å…¥æ–‡æ¡£ä¸ºç©º")
            raise HTTPException(status_code=400, detail="è¾“å…¥æ–‡æ¡£ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"âœ… è¾“å…¥éªŒè¯é€šè¿‡ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        json_input = [
            {
                "documentId": doc.documentId,
                "page": doc.page,
                "content": doc.content
            }
            for doc in documents
        ]
        
        logger.info(f"ğŸ”„ è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼å®Œæˆ")
        
        # æ‰§è¡Œå¼‚æ­¥åˆ†æ
        duplicate_results = await deduplication_service.analyze_documents(json_input)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"âœ… åˆ†æå®Œæˆï¼Œå‘ç° {len(duplicate_results)} å¯¹é‡å¤å†…å®¹ï¼Œè€—æ—¶ {processing_time:.2f}ç§’")
        
        return ApiResponse(
            success=True,
            message=f"åˆ†æå®Œæˆï¼Œå‘ç° {len(duplicate_results)} å¯¹é‡å¤å†…å®¹",
            data=duplicate_results,
            total_count=len(duplicate_results),
            processing_time=processing_time
        )
        
    except HTTPException as he:
        logger.error(f"âŒ HTTPå¼‚å¸¸: {he.detail}")
        raise he
    except Exception as e:
        logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ApiResponse(
            success=False,
            message=f"åˆ†æå¤±è´¥: {str(e)}",
            data=None,
            total_count=0,
            processing_time=processing_time
        )


@app.post("/api/v2/debug/toggle")
async def toggle_debug_mode(enable: Optional[bool] = None):
    """åŠ¨æ€åˆ‡æ¢è°ƒè¯•æ¨¡å¼
    
    Args:
        enable: trueå¼€å¯è°ƒè¯•ï¼Œfalseå…³é—­è°ƒè¯•ï¼Œä¸ä¼ å‚æ•°åˆ™åˆ‡æ¢å½“å‰çŠ¶æ€
    """
    import os
    
    current_state = config.debug_request_body
    
    if enable is None:
        # åˆ‡æ¢å½“å‰çŠ¶æ€
        new_state = not current_state
    else:
        new_state = enable
    
    # åŠ¨æ€ä¿®æ”¹ç¯å¢ƒå˜é‡
    os.environ["DEBUG_REQUEST_BODY"] = "true" if new_state else "false"
    
    logger.info(f"ğŸ”§ è°ƒè¯•æ¨¡å¼å·²{'å¼€å¯' if new_state else 'å…³é—­'} (åŸçŠ¶æ€: {'å¼€å¯' if current_state else 'å…³é—­'})")
    
    return {
        "status": "success",
        "message": f"è°ƒè¯•æ¨¡å¼å·²{'å¼€å¯' if new_state else 'å…³é—­'}",
        "previous_state": current_state,
        "current_state": new_state,
        "usage": {
            "å¼€å¯è°ƒè¯•": "POST /api/v2/debug/toggle?enable=true",
            "å…³é—­è°ƒè¯•": "POST /api/v2/debug/toggle?enable=false", 
            "åˆ‡æ¢çŠ¶æ€": "POST /api/v2/debug/toggle"
        }
    }


@app.get("/api/v2/debug/status")
async def get_debug_status():
    """è·å–å½“å‰è°ƒè¯•çŠ¶æ€"""
    return {
        "debug_request_body": config.debug_request_body,
        "message": f"è¯·æ±‚ä½“è°ƒè¯•æ—¥å¿—å½“å‰{'å·²å¼€å¯' if config.debug_request_body else 'å·²å…³é—­'}",
        "controls": {
            "å¼€å¯è°ƒè¯•": "POST /api/v2/debug/toggle?enable=true",
            "å…³é—­è°ƒè¯•": "POST /api/v2/debug/toggle?enable=false",
            "åˆ‡æ¢çŠ¶æ€": "POST /api/v2/debug/toggle"
        }
    }


@app.post("/api/v2/debug")
async def debug_request(request: Request):
    """è°ƒè¯•ç«¯ç‚¹ - è®°å½•åŸå§‹è¯·æ±‚ä½“ç”¨äºè°ƒè¯•422é”™è¯¯"""
    try:
        # è·å–åŸå§‹è¯·æ±‚ä½“
        request_body = await request.body()
        request_text = request_body.decode('utf-8')
        
        logger.info("=" * 60)
        logger.info("ğŸ” è°ƒè¯•è¯·æ±‚ä¿¡æ¯")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š è¯·æ±‚å¤´: {dict(request.headers)}")
        logger.info(f"ğŸ¯ Content-Type: {request.headers.get('content-type', 'Not Set')}")
        logger.info(f"ğŸ“ è¯·æ±‚ä½“é•¿åº¦: {len(request_body)} å­—èŠ‚")
        logger.info(f"ğŸ“„ åŸå§‹è¯·æ±‚ä½“:")
        logger.info(request_text)
        logger.info("=" * 60)
        
        # å°è¯•è§£æJSON
        try:
            parsed_json = json.loads(request_text)
            logger.info(f"âœ… JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_json)}")
            if isinstance(parsed_json, list):
                logger.info(f"ğŸ“Š æ•°ç»„é•¿åº¦: {len(parsed_json)}")
                if parsed_json:
                    logger.info(f"ğŸ¯ ç¬¬ä¸€ä¸ªå…ƒç´ : {parsed_json[0]}")
                    logger.info(f"ğŸ”‘ ç¬¬ä¸€ä¸ªå…ƒç´ çš„é”®: {list(parsed_json[0].keys()) if isinstance(parsed_json[0], dict) else 'Not a dict'}")
            elif isinstance(parsed_json, dict):
                logger.info(f"ğŸ”‘ å¯¹è±¡çš„é”®: {list(parsed_json.keys())}")
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
        
        return {
            "status": "debug_success",
            "message": "è¯·æ±‚ä¿¡æ¯å·²è®°å½•åˆ°æ—¥å¿—",
            "content_length": len(request_body),
            "content_type": request.headers.get('content-type', 'Not Set')
        }
        
    except Exception as e:
        logger.error(f"âŒ è°ƒè¯•ç«¯ç‚¹å¼‚å¸¸: {e}")
        return {
            "status": "debug_error",
            "message": f"è°ƒè¯•å¤±è´¥: {str(e)}"
        }


@app.get("/api/v2/status")
async def get_status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    return {
        "service": "Document Deduplication API",
        "version": "2.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "database": "connected"
    }


@app.post("/api/v2/test")
async def test_with_sample_data(request: Request):
    """ä½¿ç”¨ç¤ºä¾‹æ•°æ®æµ‹è¯•API - å¼‚æ­¥ç‰ˆæœ¬"""
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = [
        {
            "documentId": 1,
            "page": 1,
            "content": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ã€‚\næœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚\næ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ¨¡å¼è¯†åˆ«ã€‚"
        },
        {
            "documentId": 2,
            "page": 1,
            "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚\né€šè¿‡ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œæœºå™¨å¯ä»¥åœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹æ‰§è¡Œä»»åŠ¡ã€‚\næ·±åº¦å­¦ä¹ å±äºæœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¯†åˆ«æ¨¡å¼ã€‚"
        },
        {
            "documentId": 3,
            "page": 1,
            "content": "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åº”ç”¨é¢†åŸŸä¹‹ä¸€ã€‚\nè¯­éŸ³è¯†åˆ«æŠ€æœ¯å·²ç»å¹¿æ³›åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ä¸­ã€‚\næ¨èç³»ç»Ÿåˆ©ç”¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–å†…å®¹ã€‚"
        }
    ]
    
    # è½¬æ¢ä¸ºDocumentInputå¯¹è±¡
    documents = [DocumentInput(**item) for item in test_data]
    
    # è°ƒç”¨å¼‚æ­¥åˆ†ææ¥å£
    return await analyze_documents(request, documents)
