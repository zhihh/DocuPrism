"""
æ–‡æ¡£å¤„ç†å™¨
è´Ÿè´£æ–‡æ¡£çš„åˆ†å‰²ã€å‘é‡åŒ–ç­‰é¢„å¤„ç†å·¥ä½œ
"""

import os
import time
from typing import List, Tuple, Dict
from openai import OpenAI
from langchain_experimental.text_splitter import SemanticChunker
from langchain_core.embeddings import Embeddings
from langchain.schema import Document

from ..models.api_models import DocumentInput
from ..models.data_models import TextSegment, DocumentData
from ..utils.unified_logger import UnifiedLogger

logger = UnifiedLogger.get_logger(__name__)


class CustomEmbeddings(Embeddings):
    """è‡ªå®šä¹‰åµŒå…¥ç±»ï¼Œå…¼å®¹é˜¿é‡Œäº‘DashScope API"""
    
    def __init__(self, client: OpenAI, model: str, dimensions: int = 1024):
        self.client = client
        self.model = model
        self.dimensions = dimensions
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥å¤šä¸ªæ–‡æ¡£"""
        all_embeddings = []
        batch_size = 10
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch_texts,
                    dimensions=self.dimensions,
                    encoding_format="float"
                )
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
            except Exception as e:
                logger.error(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                # è¿”å›é›¶å‘é‡ä½œä¸ºå›é€€
                for _ in batch_texts:
                    all_embeddings.append([0.0] * self.dimensions)
        
        return all_embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=[text],
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"æŸ¥è¯¢åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºå›é€€
            return [0.0] * self.dimensions


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            base_url=os.environ.get("OPENAI_BASE_URL")
        )
        # ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥ç±»ï¼Œå…¼å®¹é˜¿é‡Œäº‘DashScope API
        embeddings = CustomEmbeddings(
            client=self.client,
            model=os.environ.get("EMBEDDING_MODEL_NAME", "text-embedding-v4"),
            dimensions=1024
        )
        self.text_splitter = SemanticChunker(
            embeddings=embeddings,
            breakpoint_threshold_type="percentile",  # ä½¿ç”¨ç™¾åˆ†ä½æ•°é˜ˆå€¼
            breakpoint_threshold_amount=95,  # 95%ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
            buffer_size=1,  # ç¼“å†²åŒºå¤§å°
            sentence_split_regex=r'(?<=[ã€‚ï¼ï¼Ÿï¼›])\s*',  # ä¸­æ–‡å¥å­åˆ†å‰²æ­£åˆ™
        )
        # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹å
        self.embedding_model_name = os.environ.get("EMBEDDING_MODEL_NAME", "text-embedding-v4")
    
    def process_json_documents(self, json_data: List[Dict]) -> Tuple[List[DocumentData], List[DocumentInput]]:
        """å¤„ç†JSONæ ¼å¼çš„æ–‡æ¡£æ•°æ®ï¼Œè¿”å›æŒ‰docä¸ºå•ä½çš„æ•°æ®å’ŒåŸå§‹è¾“å…¥"""
        start_time = time.time()
        logger.info(f"ğŸ“¥ å¼€å§‹å¤„ç†JSONæ–‡æ¡£æ•°æ®ï¼Œè¾“å…¥é¡¹ç›®æ•°: {len(json_data)}")
        
        documents_by_id = {}
        original_inputs = []
        
        # æŒ‰æ–‡æ¡£IDåˆ†ç»„ï¼Œåˆå¹¶åŒä¸€æ–‡æ¡£çš„æ‰€æœ‰é¡µé¢
        for item_idx, item in enumerate(json_data):
            doc_input = DocumentInput(
                documentId=item["documentId"],
                page=item["page"],
                content=item["content"]
            )
            original_inputs.append(doc_input)
            
            doc_id = doc_input.documentId
            if doc_id not in documents_by_id:
                documents_by_id[doc_id] = {
                    'pages': {},
                    'content_parts': []
                }
                logger.info(f"ğŸ“„ å‘ç°æ–°æ–‡æ¡£: {doc_id}")
            
            documents_by_id[doc_id]['pages'][doc_input.page] = doc_input.content
            documents_by_id[doc_id]['content_parts'].append(doc_input.content)
            
            logger.info(f"ğŸ“‘ å¤„ç†æ–‡æ¡£é¡µé¢: {doc_id} ç¬¬{doc_input.page}é¡µ, å†…å®¹é•¿åº¦: {len(doc_input.content)}å­—ç¬¦")
        
        # åˆ›å»ºDocumentDataå¯¹è±¡
        document_data_list = []
        for doc_id, data in documents_by_id.items():
            combined_content = "\n\n".join(data['content_parts'])
            doc_data = DocumentData(
                document_id=doc_id,
                content=combined_content,
                pages=data['pages']
            )
            document_data_list.append(doc_data)
            
            logger.info(f"ğŸ“‹ åˆå¹¶æ–‡æ¡£: {doc_id}, æ€»é¡µæ•°: {len(data['pages'])}, åˆå¹¶åé•¿åº¦: {len(combined_content)}å­—ç¬¦")
        
        process_time = time.time() - start_time
        logger.info(f"âœ… JSONæ–‡æ¡£å¤„ç†å®Œæˆï¼Œè€—æ—¶: {process_time:.3f}ç§’ï¼Œç”Ÿæˆ {len(document_data_list)} ä¸ªæ–‡æ¡£å¯¹è±¡")
        
        return document_data_list, original_inputs
    
    def segment_documents(self, document_inputs: List[DocumentInput]) -> List[TextSegment]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§åˆ†å‰²æ–‡æ¡£ï¼Œæ”¯æŒè·¨é¡µé¢çš„æ™ºèƒ½åˆ†å—"""
        start_time = time.time()
        logger.info(f"ğŸ” å¼€å§‹è¯­ä¹‰åˆ†å‰²æ–‡æ¡£ï¼Œè¾“å…¥é¡µé¢æ•°: {len(document_inputs)}")
        
        all_segments = []
        
        # æŒ‰æ–‡æ¡£IDåˆ†ç»„
        docs_by_id = {}
        for doc_input in document_inputs:
            if doc_input.documentId not in docs_by_id:
                docs_by_id[doc_input.documentId] = []
            docs_by_id[doc_input.documentId].append(doc_input)
        
        logger.info(f"ğŸ“Š åˆ†ç»„ç»“æœ: {len(docs_by_id)} ä¸ªä¸åŒæ–‡æ¡£")
        
        # å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œè¯­ä¹‰åˆ†å‰²
        for doc_idx, (doc_id, doc_pages) in enumerate(docs_by_id.items(), 1):
            doc_start_time = time.time()
            logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£ {doc_idx}/{len(docs_by_id)}: {doc_id}, é¡µé¢æ•°: {len(doc_pages)}")
            
            # æŒ‰é¡µç æ’åº
            doc_pages.sort(key=lambda x: x.page)
            
            # åˆå¹¶æ‰€æœ‰é¡µé¢å†…å®¹ï¼Œè®°å½•é¡µé¢è¾¹ç•Œ
            combined_content = ""
            page_boundaries = []  # [(start_pos, end_pos, page_num), ...]
            current_pos = 0
            
            for doc_page in doc_pages:
                if doc_page.content:
                    start_pos = current_pos
                    end_pos = current_pos + len(doc_page.content)
                    page_boundaries.append((start_pos, end_pos, doc_page.page))
                    combined_content += doc_page.content + "\n\n"
                    current_pos = len(combined_content)
                    logger.info(f"  ğŸ“‘ é¡µé¢ {doc_page.page}: {len(doc_page.content)} å­—ç¬¦, ä½ç½® {start_pos}-{end_pos}")
            
            if not combined_content.strip():
                logger.warning(f"âš ï¸ æ–‡æ¡£ {doc_id} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡åˆ†å‰²")
                continue
            
            logger.info(f"ğŸ“ æ–‡æ¡£ {doc_id} åˆå¹¶åæ€»é•¿åº¦: {len(combined_content)} å­—ç¬¦")
            
            # ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨è¿›è¡Œåˆ†å‰²
            try:
                chunk_start_time = time.time()
                chunks = self.text_splitter.split_text(combined_content)
                chunk_time = time.time() - chunk_start_time
                
                logger.info(f"ğŸ§  æ–‡æ¡£ {doc_id} è¯­ä¹‰åˆ†å‰²å®Œæˆï¼Œè€—æ—¶: {chunk_time:.2f}ç§’ï¼Œç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")
                
                # è½¬æ¢ä¸ºTextSegmentå¯¹è±¡ï¼Œç¡®å®šæ¯ä¸ªç‰‡æ®µæ‰€å±çš„é¡µé¢
                for chunk_id, chunk_content in enumerate(chunks, 1):
                    chunk_content = chunk_content.strip()
                    if not chunk_content:
                        continue
                    
                    # æ‰¾åˆ°ç‰‡æ®µåœ¨åˆå¹¶å†…å®¹ä¸­çš„ä½ç½®
                    chunk_start = combined_content.find(chunk_content)
                    if chunk_start == -1:
                        # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé¡µé¢
                        page_num = doc_pages[0].page
                        logger.warning(f"âš ï¸ ç‰‡æ®µ {chunk_id} ä½ç½®åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€é¡µ")
                    else:
                        # æ ¹æ®ä½ç½®ç¡®å®šæ‰€å±é¡µé¢ï¼ˆä½¿ç”¨ç‰‡æ®µå¼€å§‹ä½ç½®æ‰€åœ¨çš„é¡µé¢ï¼‰
                        page_num = doc_pages[0].page  # é»˜è®¤å€¼
                        for start_pos, end_pos, page in page_boundaries:
                            if start_pos <= chunk_start < end_pos:
                                page_num = page
                                break
                    
                    segment_id = f"doc_{doc_id}_page_{page_num}_semantic_chunk_{chunk_id}"
                    
                    segment = TextSegment(
                        id=segment_id,
                        content=chunk_content,
                        document_id=doc_id,
                        page=page_num,
                        chunk_id=chunk_id
                    )
                    
                    all_segments.append(segment)
                    logger.info(f"  âœ… ç‰‡æ®µ {chunk_id}: é•¿åº¦ {len(chunk_content)} å­—ç¬¦, å½’å±é¡µé¢ {page_num}")
                    
            except Exception as e:
                logger.error(f"âŒ å¯¹æ–‡æ¡£ {doc_id} è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ—¶å‡ºé”™: {e}")
                # å›é€€åˆ°ç®€å•çš„æŒ‰å¥å­åˆ†å‰²
                logger.info(f"ğŸ”„ æ–‡æ¡£ {doc_id} å›é€€åˆ°å¥å­åˆ†å‰²æ¨¡å¼")
                sentences = combined_content.split('ã€‚')
                for chunk_id, sentence in enumerate(sentences, 1):
                    sentence = sentence.strip()
                    if sentence:
                        segment_id = f"doc_{doc_id}_fallback_chunk_{chunk_id}"
                        segment = TextSegment(
                            id=segment_id,
                            content=sentence + 'ã€‚',
                            document_id=doc_id,
                            page=doc_pages[0].page,
                            chunk_id=chunk_id
                        )
                        all_segments.append(segment)
                
                logger.info(f"ğŸ“‹ æ–‡æ¡£ {doc_id} å›é€€åˆ†å‰²å®Œæˆï¼Œç”Ÿæˆ {len(sentences)} ä¸ªå¥å­ç‰‡æ®µ")
            
            doc_time = time.time() - doc_start_time
            logger.info(f"âœ… æ–‡æ¡£ {doc_id} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {doc_time:.2f}ç§’")
        
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ è¯­ä¹‰åˆ†å‰²å…¨éƒ¨å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’ï¼Œå…±ç”Ÿæˆ {len(all_segments)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        return all_segments
    
    def generate_embeddings(self, segments: List[TextSegment]) -> List[TextSegment]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        start_time = time.time()
        logger.info(f"ğŸ§  å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡ï¼Œè¾“å…¥ç‰‡æ®µæ•°: {len(segments)}")
        
        valid_segments = []
        contents = []
        
        # éªŒè¯å’Œå‡†å¤‡å†…å®¹
        for seg_idx, seg in enumerate(segments):
            if seg.content and isinstance(seg.content, str) and seg.content.strip():
                valid_segments.append(seg)
                contents.append(str(seg.content).strip())
            else:
                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆç‰‡æ®µ {seg_idx}: å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
        
        if not contents:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹ç”¨äºç”ŸæˆåµŒå…¥")
            return segments
        
        logger.info(f"ğŸ“‹ æœ‰æ•ˆç‰‡æ®µç»Ÿè®¡: {len(valid_segments)}/{len(segments)}, æ¨¡å‹: {self.embedding_model_name}")
        
        try:
            batch_size = 10
            all_embeddings = []
            total_batches = (len(contents) + batch_size - 1) // batch_size
            
            for i in range(0, len(contents), batch_size):
                batch_start_time = time.time()
                batch_idx = i // batch_size + 1
                
                batch_contents = contents[i:i + batch_size]
                logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx}/{total_batches}, ç‰‡æ®µæ•°: {len(batch_contents)}")
                
                # è®°å½•æ‰¹æ¬¡å†…å®¹ç»Ÿè®¡
                batch_lengths = [len(content) for content in batch_contents]
                avg_length = sum(batch_lengths) / len(batch_lengths)
                logger.info(f"  ğŸ“Š æ‰¹æ¬¡ç»Ÿè®¡: å¹³å‡é•¿åº¦ {avg_length:.0f} å­—ç¬¦, èŒƒå›´ {min(batch_lengths)}-{max(batch_lengths)}")
                
                response = self.client.embeddings.create(
                    model=self.embedding_model_name,
                    input=batch_contents,
                    dimensions=1024,
                    encoding_format="float"
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
                
                batch_time = time.time() - batch_start_time
                logger.info(f"  âœ… æ‰¹æ¬¡ {batch_idx} å®Œæˆï¼Œè€—æ—¶: {batch_time:.2f}ç§’, å‘é‡ç»´åº¦: {len(batch_embeddings[0])}")
            
            # å°†åµŒå…¥å‘é‡åˆ†é…ç»™ç‰‡æ®µ
            embedding_assign_time = time.time()
            for segment, embedding in zip(valid_segments, all_embeddings):
                segment.embedding = embedding
            assign_time = time.time() - embedding_assign_time
                
            total_time = time.time() - start_time
            avg_time_per_segment = total_time / len(valid_segments)
            
            logger.info(f"ğŸ‰ åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼")
            logger.info(f"  ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"    - æ€»ç‰‡æ®µæ•°: {len(valid_segments)}")
            logger.info(f"    - æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info(f"    - å¹³å‡æ¯ç‰‡æ®µ: {avg_time_per_segment:.3f}ç§’")
            logger.info(f"    - å‘é‡åˆ†é…è€—æ—¶: {assign_time:.3f}ç§’")
            logger.info(f"    - å¤„ç†é€Ÿåº¦: {len(valid_segments)/total_time:.1f} ç‰‡æ®µ/ç§’")
            
        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"âŒ ç”ŸæˆåµŒå…¥å‘é‡æ—¶å‡ºé”™ï¼Œå·²è€—æ—¶: {error_time:.2f}ç§’")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {e}")
            raise e
        
        return segments
